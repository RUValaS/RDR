% !TeX document-id = {2385fae9-28e6-4c73-97cb-b86e8670f6e7}
\documentclass[titlepage]{article}
\usepackage[upright]{fourier}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{svg}

\usepackage[colorlinks=true,linkcolor=teal]{hyperref}
\usepackage{amssymb,amsmath,minted,float,graphicx,textcomp,systeme,listings,physics,mathtools,ifthen,esvect,hyperref,fancyhdr,lastpage,marginnote,chngcntr,fancyvrb,subcaption}
\usepackage{stmaryrd}
\usepackage{minted}
\DeclareMathOperator{\e}{e}
\def\mathbi#1{\textbf{\em #1}}

\setlength\parindent{0pt}

%%%%%%%%%%%% PARAMETRES
\newcommand{\UE}{Stage}
\newcommand{\type}{Résultats}%TD-TP-COURS
\newcommand{\nb}{0}%nb 
\newcommand{\sbt}{Reconstruction dynamique d'images radioastronomiques}%soustitre
\author{Gabriel ROBERT-DAUTUN}
\date{2022}

% mise en page (header, compteur, fancy)
\pagestyle{fancy}
\lhead{\UE\, - \type\ifthenelse{\nb > 0}{\nb}{}}
\rfoot{\thepage /\pageref{LastPage}}
\lfoot{}
\cfoot{}
\renewcommand{\footrulewidth}{0.4pt}

%reset compteur section par partie
\counterwithin*{section}{part}

% permet notes + simples en marge à gauche (ig compteur de question)
\reversemarginpar
\newcommand{\mgn}[1]{\marginnote{#1}}

%sinon warning mdr
\setlength{\headheight}{13.07225pt}

%compteur questions
\newcounter{question}
\setcounter{question}{1}
\newcounter{subq}
\setcounter{subq}{1}

\newcommand{\rsubq}{\setcounter{subq}{1}}
\newcommand{\question}{\mgn{\thequestion .}\stepcounter{question}\rsubq}
\newcommand{\rstq}{\setcounter{question}{1}\rsubq}
\newcommand{\skipq}[1]{\addtocounter{question}{#1}\rsubq}
\newcommand{\subq}{\noindent\alph{subq})\stepcounter{subq}} %a modifier pour mettre dans le margin

\newcommand{\C}{\mathcal{C}} % mat correction
\newcommand{\Ht}{\widetilde{H}} % H err
\newcommand{\Hc}{\C\odot\Ht} % C o H
\newcommand{\B}{\mathcal{B}} % mat correction gain
\newcommand{\Hb}{\B\odot\Ht} % B o H
\newcommand{\hinv}[1]{#1^{\circ-1}} % inverse de hadamard
\newcommand{\fnorm}[1]{|\vert#1|\vert_{F}} % norme de Frobenius

\title{%
	\UE\, - \type\ifthenelse{\nb > 0}{\nb}{} \\
	\large \sbt}

\begin{document}
	
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{logo.PNG}
		\label{fig:logo}
	\end{figure}
	\maketitle
	
	\newpage
	\tableofcontents
	
	\newpage
	\section{Notations}
	\subsection{Produit de Hadamard}
	
	On introduit la notation $\odot$ associée au produit de Hadamard, qui effectue la multiplication élément à élément de deux matrices de même taille :
	$$
	\forall (A,B)\in\left(\mathbb{C}^{m\times n}\right)^2, \quad A\odot B\in\mathbb{C}^{m\times n} \quad\text{et}\quad \forall (i,j)\in\llbracket1;m\rrbracket\times\llbracket1;n\rrbracket,\, (A\odot B)_{i,j} = A_{i,j}\times B_{i,j}
	$$
	
	On introduit également l'inverse de Hadamard, qui inverse élément par élément une matrice
	$$
		\forall A \in \left(\mathbb{C}\backslash\{0\}\right)^{m\times n},\quad \hinv{A}\in\left(\mathbb{C}\backslash\{0\}\right)^{m\times n} \quad\text{et}\quad \forall (i,j)\in\llbracket1;m\rrbracket\times\llbracket1;n\rrbracket,\, \left(\hinv{A}\right)_{i,j} = \left(A_{i,j}\right)^{-1}
	$$ 
	
	Ainsi que la notation $\oslash$ désignant la division de Hadamard :
		$$
	\forall (A,B)\in\mathbb{C}^{m\times n}\times\left(\mathbb{C}\backslash0\right)^{m\times n}, \quad A\oslash B\in\mathbb{C}^{m\times n} \quad\text{et}\quad \forall (i,j)\in\llbracket1;m\rrbracket\times\llbracket1;n\rrbracket,\, (A\oslash B)_{i,j} = \frac{A_{i,j}}{B_{i,j}}
	$$
	
	\subsection{Norme de Frobenius et distances}
	
	On dénote $\fnorm{\bullet}$ la norme de Frobenius :
	$$
		A\in\mathcal{M}_{m,n}(\mathbb{K}) \quad \fnorm{A} := \sqrt{\tr{AA^H}} = \sqrt{
				\;\;\;\sum_{
					\mathclap{
						\substack{
							1\le i\le m \\1\le j\le n
						}
					}
				}\abs{A_{ij}}^2}
	$$
	
	Qui induit sur $\mathcal{M}_{m,n}(\mathbb{D})$ la distance normalisée $d_1$ :
	$$
		\forall(A,B)\in\left(\mathcal{M}_{m,n}(\mathbb{D})\right)^2\quad d_1(A,B) = \frac{\fnorm{A-B}}{mn}
	$$
	
	Où $\mathbb{D}$ désigne le disque unité fermé.\\
	
	Ainsi que la distance $d_2$ :
	$$
		\forall(A,B)\in\left(\mathcal{M}_{m,n}(\mathbb{K})\right)^2\quad d_2(A,B) = \sum_{
			\mathclap{
				\substack{
					1\le i\le m \\1\le j\le n
				}
			}
		}\abs{A_{ij} - B_{i,j}}
	$$
	
	\newpage
	\section{Influence individuelle de l'erreur sur les matrices}
	\subsection{Erreur sur l'image de départ}
	On introduit une erreur gaussienne sur l'image $X_0$. On observe que le fonctionnement du Kalman est bien celui attendu dans ce cas là : l'erreur dans l'image se réduit petit a petit.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\linewidth]{Perfs/Illustration.JPG}
		\caption{En haut : images réelles utilisées pour générer les données. En bas : images reconstruites. Titres : PSNR}
	\end{figure}

	On utilise une transformation de rotation de l'image de 90°. On remarque grâce au PSNR que les images reconstruites s'approchent des images réelles, mais pas parfaitement car le filtre prend en compte l'historique. Une erreur dans une image va donc se diffuser aux images suivantes (précédentes si backwards). On observe la convergence de reconstruction :
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\linewidth]{Perfs/conv02.png}
		\caption{Evolution psnr en fonction du nombre d'itérations}
	\end{figure}

	On converge en 4 itérations au plus a chaque essai : les erreurs gaussienne d'amplitude 0.2 se diffusent sur 4 itérations environ.
	Vérifié pour des amplitude de 0.1 a 1. Pour des amplitudes plus grandes (5 donc SNR=0.1) parfois 5 itérations sont nécessaires.
	
	\subsection{Erreur sur les matrices de covariance}
	
	Les matrices $\{P_k\}$ et $\{Q_k\}$ servent à calculer le gain de Kalman et donc sélectionner quelle proportion mesure/prédiction choisir pour estimer l'image. Si on introduit une erreur \emph{uniquement} sur une de ces matrices, l'image prédite et l'image mesurée seront les mêmes (car on y a pas introduit d'erreur) et donc une erreur sur la proportion n'aura aucun effet.\\
	Une erreur sur ces matrices n'introduit donc pas d'erreur sur la reconstruction de manière directe. Elle peut néanmoins diffuser une erreur déjà présente.
	
	\subsection{Erreur sur le modèle d'évolution}
	
	On suppose une erreur sur le modèle d'évolution \emph{dans le filtre de Kalman}, afin de simuler un modèle théorique imprécis. On construit les données simulées avec un modèle sans erreur.\\
	
	\subsubsection{Calculs théoriques}
	
	On suppose qu'on a à disposition un modèle d'évolution déformé 
	\begin{equation}
		A_d = A\ast\varepsilon
	\end{equation}
	
	où $\ast$ désigne l'opération de convolution. \\
	On obtient le nouveau modèle :
	\begin{equation}
		\widehat{x}_{k|k-1} = \left(A\ast\varepsilon\right)\widehat{x}_{k-1|k-1}
	\end{equation}

	La covariance de l'erreur de prédiction devient :
	\begin{align*}
		P_{k|k-1} &= E\left[\left(A_d\widehat{x}_{k-1|k-1} - x_k\right)\left(A_d\widehat{x}_{k-1|k-1} - x_k\right)^H\right]\\
		&= E\left\{
			\left[
				A_d(\widehat{x}_{k-1|k-1} - x_{k-1}) + (A_dx_{k-1} - x_k)
			\right]\left[
			A_d(\widehat{x}_{k-1|k-1} - x_{k-1}) + (A_dx_{k-1} - x_k)
			\right]^H
		\right\} \\
		&= A_dP_{k-1|k-1}Ad^T + Q_d + 0
	\end{align*}
	
	Le dernier terme s'explicant par la décorrélation des erreurs sur le modèle d'évolution et d'erreur de prédiction. On obtient finalement le modèle :
	\begin{equation}
		P_{k|k-1} = \left(A\ast\varepsilon\right)P_{k-1|k-1}\left(A\ast\varepsilon\right)^T + Q_d
	\end{equation}


	
	\subsection{Erreur sur le masque UV - Position}
	
	On cherche à caractériser l'erreur sur la matrice du plan UV $H$. Une manière naïve de faire le calcul serait d'ajouter une erreur gaussienne comme sur la matrice $A$, mais en faisant cela on obtient un modèle très instable et peu réaliste. Effectivement, les termes de cette matrices étant :
	$$
		H_{j,q} = \exp(-j\frac{2\pi}{\lambda}\Delta z_j\dotproduct\mathbf{I}_q)
	$$
	L'erreur sur cette matrice pouvant provenir de différentes sources : mauvaise mesure des distances entre les antennes, mauvais alignement des antennes donc erreur sur les directions, le modèle d'erreur est :
	$$
		H_{j,q}^{err} =  \exp(-i\frac{2\pi}{\lambda}(\Delta z_j\dotproduct\mathbf{I}_q + \varepsilon)) =  \exp(-i\frac{2\pi}{\lambda}\Delta z_j\dotproduct\mathbf{I}_q)\exp(-i\frac{2\pi}{\lambda}\varepsilon\dotproduct\mathbf{I}_q)
	$$
	
	Soit une rotation du complexe associé a chaque direction. \\
	
	On implémente donc l'erreur de la manière suivante pour quantifier l'erreur acceptable sur les positions des antennes :
	
	\begin{minted}{matlab}
poids = 1e-2;
z_err = z+ randn(size(z))*poids;
H_err = matF(J,Pix,z_err,lambda,I);
	\end{minted}

	On peut ensuite mesurer l'évolution du PSNR dans le temps, pour différents écarts possibles :
	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=0.7\linewidth]{Perfs/H5e-3}
%		\caption{Evolution du PSNR pour $<\varepsilon^2>=5$mm}
%		\label{fig:h5e-3}
%	\end{figure}
%	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=0.7\linewidth]{Perfs/He-3_1}
%		\caption{Evolution du PSNR pour $<\varepsilon^2>=1$mm}
%		\label{fig:he-3}
%	\end{figure}
%	
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=0.7\linewidth]{Perfs/He-2}
%		\caption{Evolution du PSNR pour $<\varepsilon^2>=1$cm}
%		\label{fig:he-2}
%	\end{figure}
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{Perfs/He-3_1}
			\caption{Evolution du PSNR pour $<\varepsilon^2>=1$mm}
			\label{fig:he-3}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{Perfs/H5e-3}
			\caption{Evolution du PSNR pour $<\varepsilon^2>=5$mm}
			\label{fig:h5e-3}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{Perfs/He-2}
			\caption{Evolution du PSNR pour $<\varepsilon^2>=1$cm}
			\label{fig:he-2}
		\end{subfigure}
		\caption{Mesure du psnr pour différentes erreurs sur la position}
	\end{figure}

	Les mesures obtenues ont \emph{généralement} cette forme : on obtient parfois des évolutions sans le pic au début, mais la mesure va toujours converger vers une valeur identique tant que $<\varepsilon^2>$ reste constant, en oscillant autour de cette valeur. On peut donc tracer l'évolution de l'erreur de reconstruction en fonction de $<\varepsilon^2>$ :
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{Perfs/perf_errF}
		\caption{Qualité de reconstruction en fonction de l'erreur}
	\end{figure}

	\subsubsection{Calculs théoriques}
	
	On considère la matrice $\widetilde{H}$ construite avec une erreur de mesure sur la position des antennes, c'est à dire 
	$$
		\forall j \in \llbracket1;J^2\rrbracket,\:\: \widetilde{\Delta} z_j = \Delta z_j + \boldsymbol{\varepsilon}_j
	$$
	On suppose l'erreur $\boldsymbol{\varepsilon}$ suivant une loi normale.\\
	Les termes de la matrice $\widetilde{H}$ deviennent :
	$$
		\widetilde{H}_{j,q} = \exp(-i\frac{2\pi}{\lambda}\widetilde{\Delta}z_j\dotproduct\mathbf{I}_q) = H_{j,q}\times\exp(-i\frac{2\pi}{\lambda}\boldsymbol{\varepsilon}_j\dotproduct\mathbf{I}_q)
	$$
	

	On peut donc établir une matrice de correction $\mathcal{C}$ telle que :
	$$
		H = \mathcal{C}\odot\widetilde{H}
	$$
	\newpage
	
	On peut vérifier que l'expression de $K$ reste inchangée : on exprime $P_{k|k}$ :
	\begin{align*}
		P_{k|k} &= E\left[\left(\widehat{x}_{k|k} - x_k\right)\left(\widehat{x}_{k|k} - x_k\right)^H\right] \\
		&= E\left[\left(\widehat{x}_{k|k-1} + K(y_k - H\widehat{x}_{k|k-1}) - x_k\right)\left(\widehat{x}_{k|k-1} + K(y_k - H\widehat{x}_{k|k-1}) - x_k\right)^H\right] \\
		&= E\left[\left(\widehat{x}_{k|k-1} - K(\C\odot\widetilde{H})\widehat{x}_{k|k-1} + Ky_k - x_k\right)\left(\widehat{x}_{k|k-1} - K(\C\odot\widetilde{H})\widehat{x}_{k|k-1} + Ky_k - x_k\right)^H\right] \\
		&= E\left[\left((\widehat{x}_{k|k-1}-x_k) -K(\C\odot\widetilde{H})(\widehat{x}_{k|k-1}-x_k) + K(y_k - (\C\odot\widetilde{H})x_k)\right)\left((\widehat{x}_{k|k-1}-x_k) -K(\C\odot\widetilde{H})(\widehat{x}_{k|k-1}-x_k) + K(y_k - (\C\odot\widetilde{H})x_k)\right)^H\right]
	\end{align*}
	
	On peut faire apparaître les erreurs de mesure $\varepsilon_{mes}$ et de prédiction $\varepsilon_{pred}$ :
	\begin{align*}
		\varepsilon_{mes} &= y_k - (\C\odot\widetilde{H})x_k \\
		\varepsilon_{pred} &= \widehat{x}_{k|k-1} - x_k
	\end{align*}

	Ces erreurs sont décorrelées, donc les termes correspondant seront nuls :
	$$
		E\left[\varepsilon_{mes}\varepsilon_{pred}^H\right] = 0
	$$
	
	En rappelant de plus :
	\begin{align*}
		P_{k|k-1} &= E\left[\varepsilon_{pred}\varepsilon_{pred}^H\right] \\
		R &=  E\left[\varepsilon_{mes}\varepsilon_{mes}^H\right]
	\end{align*}

	Erreurs aux indices k, on peut développer la formule de $P_{k|k}$
	
	\begin{align*}
		P_{k|k} &= P_{k|k-1} + K\left(\C\odot \widetilde{H}\right)P_{k|k-1}\left(\Hc\right)^H - P_{k|k-1}\left(K\Hc\right)^H - K\left(\Hc\right)P_{k|k-1} + KRK^H
	\end{align*}
	
	Cette forme est bien minimisée par 
	\begin{equation}
		K_k^c = P_{k|k-1}\left(\Hc\right)^H\left(\left(\Hc\right)P_{k|k-1}\left(\Hc\right)^H + R\right)^{-1}
	\end{equation}
	Forme similaire au gain $K$ calculé sans correction :
	\begin{equation}
		K_k = P_{k|k-1}\Ht^H\left(\Ht P_{k|k-1}\Ht^H + R\right)^{-1}
	\end{equation}

	Par la suite, on omettra les dépendances en $k$ en supposant $P=P_{k|k-1}$ et $K = K_k$.
	
	On pose l'erreur sur le gain :
	\begin{equation}
		\varepsilon_K = K - K^c 
	\end{equation}

	Ainsi que l'erreur d'estimation induite par l'erreur de masque :
	\begin{equation}
		\varepsilon_{\widehat{x}} = \widehat{x}_{k|k} - \widehat{x}_{k|k}^c
	\end{equation}
	
	On omettra de la même façon la dépendance en $k$. En développant :
	\begin{align*}
		\varepsilon_{\widehat{x}} &= \left[\widehat{x}_{k|k-1} + K(y_k-\Ht\widehat{x}_{k|k-1})\right] - \left[\widehat{x}_{k|k-1} + K^c\left(y_k - \left(\Hc\right)\widehat{x}_{k|k-1}\right)\right] \\
		&= \left(K-K^c\right)y_k - \left(K\Ht - K^c(\Hc)\right)\widehat{x}_{k|k-1} \\
		&= \varepsilon_K\left[y_k - \left(\Hc\right)\widehat{x}_{k|k-1}\right] + K\left(\mathbb{1}-\C\right)\odot\Ht\widehat{x}_{k|k-1} \\
		&= \varepsilon_K(y_k - (\Hc)\left(\varepsilon_{pred} + x_k\right)) + K\left(\mathbb{1}-\C\right)\odot\Ht\widehat{x}_{k|k-1} \\
		&= \varepsilon_K\left[\left(y_k - (\Hc)x_k\right) - (\Hc)\varepsilon_{pred}\right] + K\left(\mathbb{1}-\C\right)\odot\Ht\widehat{x}_{k|k-1} \\
		&= \varepsilon_K\left(\varepsilon_{mes} - (\Hc)\varepsilon_{pred}\right) + K\left(\mathbb{1}-\C\right)\odot\Ht\widehat{x}_{k|k-1}
	\end{align*}

	Où $\mathbb{1}$ désigne l'élément neutre du produit de Hadamard, la matrice dont tous les termes valent 1.\\	

	Sous l'hypothèse de l'introduction d'erreur uniquement sur $H$, la matrice $\C\odot\Ht$ est la matrice corrigée et on obtient :
	$$
		y_k = (\Hc) x_k \implies \varepsilon_{mes} = 0
	$$
	On ne suppose également pas d'erreur sur la matrice d'évolution $A$ :
	$$
		\varepsilon_{pred} = 0
	$$
	
	D'où :
	\begin{equation}\label{eq:approx}
		\varepsilon_{\widehat{x}} = K \left(\mathbb{1} - \C\right)\odot\Ht\widehat{x}_{k|k-1}
	\end{equation}

	On peut de même évaluer le différentiel sur les covariances :
	\begin{equation}
		\Delta P_{k|k} = P_{k|k} - P_{k|k}^c = \left(K^c\left(\Hc\right) - K\Ht\right)P_{k|k-1}
	\end{equation}

	De plus, si on suppose une erreur de gain sur les antennes, on peut poser les termes :
	$$
		\Ht_{j,q} = (1+\varepsilon_{j,q})H_{j,q}
	$$
	
	Où $\varepsilon_q$ suit une loi normale, et désigne une erreur sur le gain de l'antenne $j$ dans la direction $q$.\\
	On peut de même établir une matrice de correction $\B$ telle que :
	\begin{equation}
		H = \Hb
	\end{equation}

	Les erreurs et gain auront alors la même forme
	\begin{subequations} 
		\begin{equation}
			K_k^b = P\left(\Hb\right)^H\left(\left(\Hb\right)P\left(\Hb\right)^H + R\right)^{-1}
		\end{equation}
		\begin{equation}
			\varepsilon_{\widehat{x}} = K\left(\mathbb{1} - \B\right)\odot\Ht\widehat{x}_{k|k-1}
		\end{equation}
		\begin{equation}
			\Delta P_{k|k} = \left(K^b\left(\Hb\right) - K\Ht\right)P_{k|k-1}
		\end{equation}
	\end{subequations}
	
	\subsubsection{Essais expérimentaux}
	
	On effectue des essais en introduisant une erreur sur la matrice exacte. Plutôt que d'avoir une matrice erronée $\Ht$ connue on aura $H$ exacte connue et :
	$$
		\Ht = \mathcal{E}\odot H
	$$
	où $\mathcal{E} = \hinv{\C}$ ou $\mathcal{E} = \hinv{\B}$ en fonction de l'erreur que l'on souhaite introduire.
	
	\paragraph{Erreur de position}
	On suppose tout d'abord que l'on ne connaît pas exactement la position des antennes du réseau. On introduit sur $H$ une matrice de petites rotations définie par la matrice $\mathcal{R} = \hinv{\C}$ plus tôt. \\
	Pour cela, on génère des erreurs directement sur les positions des antennes, puis on peut obtenir $\C$ avec la connaissance de $H$ et $\Ht$ :
	\begin{equation}
		\C = H \oslash \Ht
	\end{equation}

	Cette division est possible car tous les termes sont de norme 1 ou proche de 1 dans le cas de petites variations.
	
	On effectue 6 réalisations de Kalman sur 8 instants temporels en introduisant une erreur sur la position des antennes. On effectue cette opération pour plusieurs erreurs et on peut observer les erreurs $\varepsilon_K$ et $\varepsilon_{\widehat{x}}$ calculées par la distance $d_1$, normalisée sur le disque unité :
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\includesvg[width=.9\linewidth]{Perfs/e_K.svg}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\includesvg[width=.9\linewidth]{Perfs/e_x.svg}
		\end{subfigure}
		\caption{Erreur induite sur les matrices par un erreur de position}
	\end{figure}

	On peut également tracer l'erreur commise sur la reconstruction en fonction de l'erreur sur la matrice K :
	
	\begin{figure}[H]
		\centering
		\includesvg[width=0.6\linewidth]{Perfs/x_fK}
		\caption{Erreur de reconstruction en fonction de l'erreur sur la matrice K}
	\end{figure}
	
	On peut également vérifier la précision de l'approximation pour obtenir l'équation (\ref{eq:approx}) : 
	
	\begin{figure}[H]
		\centering
		\includesvg[width=.7\linewidth]{Perfs/graph_approx.svg}
		\caption{Essai sur une réalisation de 10 instants}
	\end{figure}

	On peut également tracer la robustesse à l'erreur de cette approximation :
	
	\begin{figure}[H]
		\centering
		\includesvg[width=.6\linewidth]{Perfs/approx.svg}
		\caption{Erreur de reconstruction finale en fonction de l'erreur en position}
	\end{figure}

	\subsection{Erreur sur le masque UV - direction}
	
	En considérant une erreur sur la position des antennes :
	\begin{equation}
		\forall q\in \llbracket 1;Q\rrbracket,\; \widetilde{\boldsymbol{I}_q} = \boldsymbol{I}_q + \boldsymbol{\varepsilon}_q
	\end{equation}
	
	où $Q$ désigne le nombre de directions observées, on obtient un calcul extrêmement similaire à l'erreur sur les positions d'antennes. On peut donc effectuer de même des essais afin de vérifier les performances en présence d'une erreur. \\
	On effectue les essais sur 24 directions (quadrillage $4\times6$) et 36 antennes (réseau carré $6\times6$). On quantifiera d'abord l'erreur en présence d'une direction faussée, puis de 5 et enfin toutes. \\
	
	\paragraph{Erreur sur une unique direction :} 
	On utilisera dans cette partie la distance $d_2$, afin d'évaluer l'impact de la direction sur laquelle est faite l'erreur sur l'erreur de reconstruction totale, grâce à l'additivité de la distance sur les termes de la matrice évaluée.
	% a reformuler
	
	On obtient :
	
	\begin{figure}[H]
		\centering
		\includesvg[width=.6\linewidth]{Perfs/err_I_1.svg}
		\caption{Erreurs de reconstruction totale et sur la direction faussée}
	\end{figure}

	\paragraph{Erreur sur toutes les directions}
	On utilise la distance $d_1$. On introduit une erreur sur toutes les directions.
	
	\begin{figure}[H]
		\centering
		\includesvg[width=.6\linewidth]{Perfs/err_I_all.svg}
		\caption{Erreurs de reconstruction}
	\end{figure}
	
	
\end{document}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{}
	\caption{}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{image1}
		\caption{}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{image2}
		\caption{}
	\end{subfigure}
	\caption{}
\end{figure}

