<p><img src="logo.PNG" alt="image" /> <span id="fig:logo"
label="fig:logo"></span></p>
<h1 id="influence-individuelle-de-lerreur-sur-les-matrices">Influence
individuelle de lâ€™erreur sur les matrices</h1>
<h2 id="erreur-sur-limage-de-dÃ©part">Erreur sur lâ€™image de dÃ©part</h2>
<p>On introduit une erreur gaussienne sur lâ€™image <span
class="math inline"><em>X</em><sub>0</sub></span>. On observe que le
fonctionnement du Kalman est bien celui attendu dans ce cas lÃ  :
lâ€™erreur dans lâ€™image se rÃ©duit petit a petit.</p>
<figure>
<img src="Perfs/Illustration.JPG"
alt="En haut : images rÃ©elles utilisÃ©es pour gÃ©nÃ©rer les donnÃ©es. En bas : images reconstruites. Titres : PSNR" />
<figcaption aria-hidden="true">En haut : images rÃ©elles utilisÃ©es pour
gÃ©nÃ©rer les donnÃ©es. En bas : images reconstruites. Titres :
PSNR</figcaption>
</figure>
<p>On utilise une transformation de rotation de lâ€™image de 90Â°. On
remarque grÃ¢ce au PSNR que les images reconstruites sâ€™approchent des
images rÃ©elles, mais pas parfaitement car le filtre prend en compte
lâ€™historique. Une erreur dans une image va donc se diffuser aux images
suivantes (prÃ©cÃ©dentes si backwards). On observe la convergence de
reconstruction :</p>
<figure>
<img src="Perfs/conv02.png"
alt="Evolution psnr en fonction du nombre dâ€™itÃ©rations" />
<figcaption aria-hidden="true">Evolution psnr en fonction du nombre
dâ€™itÃ©rations</figcaption>
</figure>
<p>On converge en 4 itÃ©rations au plus a chaque essai : les erreurs
gaussienne dâ€™amplitude 0.2 se diffusent sur 4 itÃ©rations environ.
VÃ©rifiÃ© pour des amplitude de 0.1 a 1. Pour des amplitudes plus grandes
(5 donc SNR=0.1) parfois 5 itÃ©rations sont nÃ©cessaires.</p>
<h2 id="erreur-sur-les-matrices-de-covariance">Erreur sur les matrices
de covariance</h2>
<p>Les matrices <span
class="math inline">{<em>P</em><sub><em>k</em></sub>}</span> et <span
class="math inline">{<em>Q</em><sub><em>k</em></sub>}</span> servent Ã 
calculer le gain de Kalman et donc sÃ©lectionner quelle proportion
mesure/prÃ©diction choisir pour estimer lâ€™image. Si on introduit une
erreur <em>uniquement</em> sur une de ces matrices, lâ€™image prÃ©dite et
lâ€™image mesurÃ©e seront les mÃªmes (car on y a pas introduit dâ€™erreur) et
donc une erreur sur la proportion nâ€™aura aucun effet.<br />
Une erreur sur ces matrices nâ€™introduit donc pas dâ€™erreur sur la
reconstruction de maniÃ¨re directe. Elle peut nÃ©anmoins diffuser une
erreur dÃ©jÃ  prÃ©sente.</p>
<h2 id="erreur-sur-le-modÃ¨le-dÃ©volution">Erreur sur le modÃ¨le
dâ€™Ã©volution</h2>
<p>On suppose une erreur sur le modÃ¨le dâ€™Ã©volution <em>dans le filtre de
Kalman</em>, afin de simuler un modÃ¨le thÃ©orique imprÃ©cis. On construit
les donnÃ©es simulÃ©es avec un modÃ¨le sans erreur.<br />
</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="va">poids</span> <span class="op">=</span> <span class="fl">0.2</span><span class="op">;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="va">A</span> <span class="op">=</span> <span class="va">A</span> <span class="op">+</span> <span class="va">poids</span><span class="op">*</span><span class="va">randn</span>(<span class="va">size</span>(<span class="va">A</span>))<span class="op">;</span></span></code></pre></div>
<p>On introduit une erreur gaussienne</p>
<h2 id="erreur-sur-le-masque-uv">Erreur sur le masque UV</h2>
<p>On cherche Ã  caractÃ©riser lâ€™erreur sur la matrice du plan UV <span
class="math inline"><em>H</em></span>. Une maniÃ¨re naÃ¯ve de faire le
calcul serait dâ€™ajouter une erreur gaussienne comme sur la matrice <span
class="math inline"><em>A</em></span>, mais en faisant cela on obtient
un modÃ¨le trÃ¨s instable et peu rÃ©aliste. Effectivement, les termes de
cette matrices Ã©tant : <span class="math display">$$H_{j,q} =
\exp(-j\frac{2\pi}{\lambda}\Delta z_j\dotproduct\mathbf{I}_q)$$</span>
Lâ€™erreur sur cette matrice pouvant provenir de diffÃ©rentes sources :
mauvaise mesure des distances entre les antennes, mauvais alignement des
antennes donc erreur sur les directions, le modÃ¨le dâ€™erreur est : <span
class="math display">$$H_{j,q}^{err}
=  \exp(-i\frac{2\pi}{\lambda}(\Delta z_j\dotproduct\mathbf{I}_q +
\varepsilon)) =  \exp(-i\frac{2\pi}{\lambda}\Delta
z_j\dotproduct\mathbf{I}_q)\exp(-i\frac{2\pi}{\lambda}\varepsilon\dotproduct\mathbf{I}_q)$$</span></p>
<p>Soit une rotation du complexe associÃ© a chaque direction.<br />
On implÃ©mente donc lâ€™erreur de la maniÃ¨re suivante pour quantifier
lâ€™erreur acceptable sur les positions des antennes :</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="va">poids</span> <span class="op">=</span> <span class="fl">1e-2</span><span class="op">;</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="va">z_err</span> <span class="op">=</span> <span class="va">z</span><span class="op">+</span> <span class="va">randn</span>(<span class="va">size</span>(<span class="va">z</span>))<span class="op">*</span><span class="va">poids</span><span class="op">;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="va">H_err</span> <span class="op">=</span> <span class="va">matF</span>(<span class="va">J</span><span class="op">,</span><span class="va">Pix</span><span class="op">,</span><span class="va">z_err</span><span class="op">,</span><span class="va">lambda</span><span class="op">,</span><span class="va">I</span>)<span class="op">;</span></span></code></pre></div>
<p>On peut ensuite mesurer lâ€™Ã©volution du PSNR dans le temps, pour
diffÃ©rents Ã©carts possibles :</p>
<figure>
<img src="Perfs/He-3_1.png" id="fig:he-3"
alt="Evolution du PSNR pour &lt;\varepsilon^2&gt;=1mm" />
<figcaption aria-hidden="true">Evolution du PSNR pour <span
class="math inline">â€„&lt;â€„<em>Îµ</em><sup>2</sup>â€„&gt;â€„â€„=â€„1</span>mm</figcaption>
</figure>
<figure>
<img src="Perfs/H5e-3.png" id="fig:h5e-3"
alt="Evolution du PSNR pour &lt;\varepsilon^2&gt;=5mm" />
<figcaption aria-hidden="true">Evolution du PSNR pour <span
class="math inline">â€„&lt;â€„<em>Îµ</em><sup>2</sup>â€„&gt;â€„â€„=â€„5</span>mm</figcaption>
</figure>
<figure>
<img src="Perfs/He-2.png" id="fig:he-2"
alt="Evolution du PSNR pour &lt;\varepsilon^2&gt;=1cm" />
<figcaption aria-hidden="true">Evolution du PSNR pour <span
class="math inline">â€„&lt;â€„<em>Îµ</em><sup>2</sup>â€„&gt;â€„â€„=â€„1</span>cm</figcaption>
</figure>
<p>Les mesures obtenues ont <em>gÃ©nÃ©ralement</em> cette forme : on
obtient parfois des Ã©volutions sans le pic au dÃ©but, mais la mesure va
toujours converger vers une valeur identique tant que <span
class="math inline">â€„&lt;â€„<em>Îµ</em><sup>2</sup>&gt;</span> reste
constant, en oscillant autour de cette valeur. On peut donc tracer
lâ€™Ã©volution de lâ€™erreur de reconstruction en fonction de <span
class="math inline">â€„&lt;â€„<em>Îµ</em><sup>2</sup>&gt;</span> :</p>
<figure>
<img src="Perfs/perf_errF.jpg"
alt="QualitÃ© de reconstruction en fonction de lâ€™erreur" />
<figcaption aria-hidden="true">QualitÃ© de reconstruction en fonction de
lâ€™erreur</figcaption>
</figure>
<h3 id="calculs-thÃ©oriques">Calculs thÃ©oriques</h3>
<p>On introduit la notation <span class="math inline">âŠ™</span> associÃ©e
au produit de Hadamard, qui effectue la multiplication Ã©lÃ©ment Ã  Ã©lÃ©ment
de deux matrices de mÃªme taille : <span
class="math display">âˆ€(<em>A</em>,<em>B</em>)â€„âˆˆâ€„(â„‚<sup><em>m</em>â€…Ã—â€…<em>n</em></sup>)<sup>2</sup>,â€Šâ€<em>A</em>â€…âŠ™â€…<em>B</em>â€„âˆˆâ€„â„‚<sup><em>m</em>â€…Ã—â€…<em>n</em></sup>â€Šâ€etâ€Šâ€âˆ€(<em>i</em>,<em>j</em>)â€„âˆˆâ€„âŸ¦1;â€†<em>m</em>âŸ§â€…Ã—â€…âŸ¦1;â€†<em>n</em>âŸ§,â€†(<em>A</em>âŠ™<em>B</em>)<sub><em>i</em>,â€†<em>j</em></sub>â€„=â€„<em>A</em><sub><em>i</em>,â€†<em>j</em></sub>â€…Ã—â€…<em>B</em><sub><em>i</em>,â€†<em>j</em></sub></span></p>
<p>On considÃ¨re la matrice <span class="math inline"><em>HÌƒ</em></span>
construite avec une erreur de mesure sur la position des antennes, câ€™est
Ã  dire <span
class="math display">âˆ€<em>j</em>â€„âˆˆâ€„âŸ¦1;â€†<em>J</em><sup>2</sup>âŸ§,â€†<em>Î”Ìƒ</em><em>z</em><sub><em>j</em></sub>â€„=â€„<em>Î”</em><em>z</em><sub><em>j</em></sub>â€…+â€…<strong>Îµ</strong><sub><em>j</em></sub></span>
On suppose lâ€™erreur <span class="math inline"><strong>Îµ</strong></span>
suivant une loi normale.<br />
Les termes de la matrice <span class="math inline"><em>HÌƒ</em></span>
deviennent : <span class="math display">$$\widetilde{H}_{j,q} =
\exp(-i\frac{2\pi}{\lambda}\widetilde{\Delta}z_j\dotproduct\mathbf{I}_q)
=
H_{j,q}\times\exp(-i\frac{2\pi}{\lambda}\boldsymbol{\varepsilon}_j\dotproduct\mathbf{I}_q)$$</span></p>
<p>On peut donc Ã©tablir une matrice de correction <span
class="math inline">ğ’</span> telle que : <span
class="math display"><em>H</em>â€„=â€„ğ’â€…âŠ™â€…<em>HÌƒ</em></span></p>
<p>On peut vÃ©rifier que lâ€™expression de <span
class="math inline"><em>K</em></span> reste inchangÃ©e : on exprime <span
class="math inline"><em>P</em><sub><em>k</em>|<em>k</em></sub></span> :
<span class="math display">$$\begin{aligned}
        P_{k|k} &amp;= E\left[\left(\widehat{x}_{k|k} -
x_k\right)\left(\widehat{x}_{k|k} - x_k\right)^H\right] \\
        &amp;= E\left[\left(\widehat{x}_{k|k-1} + K(y_k -
H\widehat{x}_{k|k-1}) - x_k\right)\left(\widehat{x}_{k|k-1} + K(y_k -
H\widehat{x}_{k|k-1}) - x_k\right)^H\right] \\
        &amp;= E\left[\left(\widehat{x}_{k|k-1} -
K(\mathcal{C}\odot\widetilde{H})\widehat{x}_{k|k-1} + Ky_k -
x_k\right)\left(\widehat{x}_{k|k-1} -
K(\mathcal{C}\odot\widetilde{H})\widehat{x}_{k|k-1} + Ky_k -
x_k\right)^H\right] \\
        &amp;= E\left[\left((\widehat{x}_{k|k-1}-x_k)
-K(\mathcal{C}\odot\widetilde{H})(\widehat{x}_{k|k-1}-x_k) + K(y_k -
(\mathcal{C}\odot\widetilde{H})x_k)\right)\left((\widehat{x}_{k|k-1}-x_k)
-K(\mathcal{C}\odot\widetilde{H})(\widehat{x}_{k|k-1}-x_k) + K(y_k -
(\mathcal{C}\odot\widetilde{H})x_k)\right)^H\right]
    \end{aligned}$$</span></p>
<p>On peut faire apparaÃ®tre les erreurs de mesure <span
class="math inline"><em>Îµ</em><sub><em>m</em><em>e</em><em>s</em></sub></span>
et de prÃ©diction <span
class="math inline"><em>Îµ</em><sub><em>p</em><em>r</em><em>e</em><em>d</em></sub></span>
: <span class="math display">$$\begin{aligned}
        \varepsilon_{mes} &amp;= y_k -
(\mathcal{C}\odot\widetilde{H})x_k \\
        \varepsilon_{pred} &amp;= \widehat{x}_{k|k-1} - x_k
    \end{aligned}$$</span></p>
<p>Ces erreurs sont dÃ©correlÃ©es, donc les termes correspondant seront
nuls : <span
class="math display"><em>E</em>[<em>Îµ</em><sub><em>m</em><em>e</em><em>s</em></sub><em>Îµ</em><sub><em>p</em><em>r</em><em>e</em><em>d</em></sub><sup><em>H</em></sup>]â€„=â€„0</span></p>
<p>En rappelant de plus : <span class="math display">$$\begin{aligned}
        P_{k|k-1} &amp;=
E\left[\varepsilon_{pred}\varepsilon_{pred}^H\right] \\
        R &amp;=  E\left[\varepsilon_{mes}\varepsilon_{mes}^H\right]
    \end{aligned}$$</span></p>
<p>Erreurs aux indices k, on peut dÃ©velopper la formule de <span
class="math inline"><em>P</em><sub><em>k</em>|<em>k</em></sub></span></p>
<p><span class="math display">$$\begin{aligned}
        P_{k|k} &amp;= P_{k|k-1} + K\left(\mathcal{C}\odot
\widetilde{H}\right)P_{k|k-1}\left(\mathcal{C}\odot\widetilde{H}\right)^H
- P_{k|k-1}\left(K\mathcal{C}\odot\widetilde{H}\right)^H -
K\left(\mathcal{C}\odot\widetilde{H}\right)P_{k|k-1} + KRK^H
    \end{aligned}$$</span></p>
<p>Cette forme est bien minimisÃ©e par <span
class="math display"><em>K</em><sub><em>k</em></sub><sup><em>c</em></sup>â€„=â€„<em>P</em><sub><em>k</em>|<em>k</em>â€…âˆ’â€…1</sub>(ğ’âŠ™<em>HÌƒ</em>)<sup><em>H</em></sup>((ğ’âŠ™<em>HÌƒ</em>)<em>P</em><sub><em>k</em>|<em>k</em>â€…âˆ’â€…1</sub>(ğ’âŠ™<em>HÌƒ</em>)<sup><em>H</em></sup>+<em>R</em>)<sup>âˆ’1</sup></span>
Forme similaire au gain <span class="math inline"><em>K</em></span>
calculÃ© sans correction : <span
class="math display"><em>K</em><sub><em>k</em></sub>â€„=â€„<em>P</em><sub><em>k</em>|<em>k</em>â€…âˆ’â€…1</sub><em>HÌƒ</em><sup><em>H</em></sup>(<em>HÌƒ</em><em>P</em><sub><em>k</em>|<em>k</em>â€…âˆ’â€…1</sub><em>HÌƒ</em><sup><em>H</em></sup>+<em>R</em>)<sup>âˆ’1</sup></span></p>
<p>Par la suite, on omettra les dÃ©pendances en <span
class="math inline"><em>k</em></span> en supposant <span
class="math inline"><em>P</em>â€„=â€„<em>P</em><sub><em>k</em>|<em>k</em>â€…âˆ’â€…1</sub></span>
et <span
class="math inline"><em>K</em>â€„=â€„<em>K</em><sub><em>k</em></sub></span>.</p>
<p>On pose lâ€™erreur sur le gain : <span
class="math display"><em>Îµ</em><sub><em>K</em></sub>â€„=â€„<em>K</em><sup><em>c</em></sup>â€…âˆ’â€…<em>K</em></span></p>
<p>Ainsi que lâ€™erreur dâ€™estimation induite par lâ€™erreur de masque :
<span
class="math display"><em>Îµ</em><sub><em>xÌ‚</em></sub>â€„=â€„<em>xÌ‚</em><sub><em>k</em>|<em>k</em></sub>â€…âˆ’â€…<em>xÌ‚</em><sub><em>k</em>|<em>k</em></sub><sup><em>c</em></sup></span></p>
<p>On omettra de la mÃªme faÃ§on la dÃ©pendance en <span
class="math inline"><em>k</em></span>. En dÃ©veloppant : <span
class="math display">$$\begin{aligned}
        \varepsilon_{\widehat{x}} &amp;= \left[\widehat{x}_{k|k-1} +
K(y_k-\widetilde{H}\widehat{x}_{k|k-1})\right] -
\left[\widehat{x}_{k|k-1} + K^c\left(y_k -
\left(\mathcal{C}\odot\widetilde{H}\right)\widehat{x}_{k|k-1}\right)\right]
\\
        &amp;= \left(K-K^c\right)y_k - \left(K\widetilde{H}-
K^c(\mathcal{C}\odot\widetilde{H})\right)\widehat{x}_{k|k-1}
    \end{aligned}$$</span></p>
